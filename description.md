
# 1. Описание реализации

1.1.  
В связи с ограничением по времени, минимализируем задачу.  
Для тестирования загрузки берем [демо базу PostgresPro](https://postgrespro.ru/education/demodb)  
HDFS поднимаем в отдельном контейнере  
Базы данных Metadata и status в отдельном Postgres  
RawData и ODS в Spark  
Параметры тестового подключения/запроса хранятся в виде файла json  
     
1.2.  
На первом этапе прорабатываем минимальный функционал - пробрасывание данных из Postgress в HDFS  
Все функции расширения прописываем как ToDo  
Логирование в файл. Строка: "ДатаВремя произвольное сообщение (описание что за действие/статус логируется в свободной форме)"  


# 2. Структура

2.1. Структура кода:
> | //  
> |-src //каталог со всем кодом  
> | |- log // логирование  
> | |- metadata // взаимодействие с метаданными  
> | |- params // конфигурация и параметры демо-запроса  
> | |- source // запрос к источнику  
> | |- storage // работа со spark (RawData & ODS)  
> | |- target // загрузка в HDFS  
> | main.py //запуск кода  

2.2. Параметры

имеется 2 файла с параметрами:  
- src/params/config.json - системные параметры приложения (расположение логов, коннекты к служебным БД)
  - _log_file_ - расположение файла для записи логов
  - _meta_database_ - параметры подключения к базе метаданных
- src/params/query.json - демо-параметры соединения и запроса к БД
  - _source_database_ - параметры коннекта к источнику данных
  - _target_ - параметры цели, в том числе тип загрузки (дополнение/перезапись), формат файлов и т.п. 
  - _metadata_ - мэппинг полей и типов источника и цели
  - _params_ - параметры запроса. в т.ч. ограничения (последние 100 записей, начиная с такого-то числа, или все после указанного id)
